===========================================================
                                      .___ __  __   
          _________________  __ __  __| _/|__|/  |_ 
         / ___\_` __ \__  \ |  |  \/ __ | | \\_  __\
        / /_/  >  | \// __ \|  |  / /_/ | |  ||  |  
        \___  /|__|  (____  /____/\____ | |__||__|  
       /_____/            \/           \/           
              grep rough audit - static analysis tool
                  v3.5 written by @Wireghoul
=================================[justanotherhacker.com]===
[35m[K/tmp/gpt_academic/crazy_functions/test_project/latex/attention/model_architecture.tex[m[K[36m[K-[m[K[32m[K17[m[K[36m[K-[m[K
[35m[K/tmp/gpt_academic/crazy_functions/test_project/latex/attention/model_architecture.tex[m[K[36m[K:[m[K[32m[K18[m[K[36m[K:[m[K\paragraph{Encoder:}The encoder is composed of a stack of $N=6$ identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.   We employ a residual connection \citep{he2016deep} around each of the two sub-layers, followed by layer normalization \cite{layernorm2016}.  That is, the output of each sub-layer is $\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$, [01;31m[Kwhere $\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.  To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $\dmodel=512$.[m[K
[35m[K/tmp/gpt_academic/crazy_functions/test_project/latex/attention/model_architecture.tex[m[K[36m[K-[m[K[32m[K19[m[K[36m[K-[m[K
[36m[K##############################################[m[K
[35m[K/tmp/gpt_academic/crazy_functions/test_project/latex/attention/model_architecture.tex[m[K[36m[K-[m[K[32m[K52[m[K[36m[K-[m[K
[35m[K/tmp/gpt_academic/crazy_functions/test_project/latex/attention/model_architecture.tex[m[K[36m[K:[m[K[32m[K53[m[K[36m[K:[m[KWhile for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ \citep{DBLP:journals/corr/BritzGLL17}. We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions [01;31m[Kwhere it has extremely small gradients  \footnote{To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  Then their dot product, $q \cdot k = \sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.}. To counteract this effect, we scale the dot products by $\frac{1}{\sqrt{d_k}}$.[m[K
[35m[K/tmp/gpt_academic/crazy_functions/test_project/latex/attention/model_architecture.tex[m[K[36m[K-[m[K[32m[K54[m[K[36m[K-[m[K
[36m[K##############################################[m[K
[35m[K/tmp/gpt_academic/multi_language.py[m[K[36m[K-[m[K[32m[K217[m[K[36m[K-[m[K            try:
[35m[K/tmp/gpt_academic/multi_language.py[m[K[36m[K:[m[K[32m[K218[m[K[36m[K:[m[K                res_before_trans = [01;31m[Keval(result[i-1])[m[K
[35m[K/tmp/gpt_academic/multi_language.py[m[K[36m[K:[m[K[32m[K219[m[K[36m[K:[m[K                res_after_trans = [01;31m[Keval(result[i])[m[K
[35m[K/tmp/gpt_academic/multi_language.py[m[K[36m[K-[m[K[32m[K220[m[K[36m[K-[m[K                if len(res_before_trans) != len(res_after_trans): 
[36m[K##############################################[m[K
[35m[K/tmp/gpt_academic/multi_language.py[m[K[36m[K-[m[K[32m[K231[m[K[36m[K-[m[K                print('GPT answers with unexpected format, some words may not be translated, but you can try again later to increase translation coverage.')
[35m[K/tmp/gpt_academic/multi_language.py[m[K[36m[K:[m[K[32m[K232[m[K[36m[K:[m[K                res_before_trans = [01;31m[Keval(result[i-1])[m[K
[35m[K/tmp/gpt_academic/multi_language.py[m[K[36m[K-[m[K[32m[K233[m[K[36m[K-[m[K                for a in res_before_trans:
[36m[K##############################################[m[K
[35m[K/tmp/gpt_academic/toolbox.py[m[K[36m[K-[m[K[32m[K561[m[K[36m[K-[m[K        elif isinstance(default_value, dict):
[35m[K/tmp/gpt_academic/toolbox.py[m[K[36m[K:[m[K[32m[K562[m[K[36m[K:[m[K            r = [01;31m[Keval(env_arg)[m[K
[35m[K/tmp/gpt_academic/toolbox.py[m[K[36m[K-[m[K[32m[K563[m[K[36m[K-[m[K        elif isinstance(default_value, list):
[35m[K/tmp/gpt_academic/toolbox.py[m[K[36m[K:[m[K[32m[K564[m[K[36m[K:[m[K            r = [01;31m[Keval(env_arg)[m[K
[35m[K/tmp/gpt_academic/toolbox.py[m[K[36m[K-[m[K[32m[K565[m[K[36m[K-[m[K        elif default_value is None:
[35m[K/tmp/gpt_academic/toolbox.py[m[K[36m[K-[m[K[32m[K566[m[K[36m[K-[m[K            assert arg == "proxies"
[35m[K/tmp/gpt_academic/toolbox.py[m[K[36m[K:[m[K[32m[K567[m[K[36m[K:[m[K            r = [01;31m[Keval(env_arg)[m[K
[35m[K/tmp/gpt_academic/toolbox.py[m[K[36m[K-[m[K[32m[K568[m[K[36m[K-[m[K        else:
